import dataclasses
import logging
import os
from pathlib import Path
import re
import gc
import math
from string import Template
from http.server import HTTPServer, SimpleHTTPRequestHandler
from threading import Thread
from pathlib import Path
import time

from langchain.agents import initialize_agent, AgentType
from tqdm import tqdm
from typing import Optional

import openai
from click.core import batch
from dotenv import load_dotenv
from openai import OpenAI
from dataclasses import dataclass, asdict, fields
from typing import List, Dict
import json
from pathlib import Path
from playwright.sync_api import sync_playwright
import shutil

import pandas as pd
import lightgbm as lgb

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.cluster import KMeans

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain_core.tools import Tool
from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun
from langchain_community.tools.google_serper import GoogleSerperRun
from langchain.utilities import GoogleSerperAPIWrapper
from langchain_community.tools.playwright.utils import create_async_playwright_browser
from langchain_community.tools.playwright.utils import create_sync_playwright_browser
from langchain_community.agent_toolkits import PlayWrightBrowserToolkit
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI

from common.module import DATA_PATH, brandStatsVo, datStoreVo, datSalesVo, OUTPUT_PATH, LOGO_PATH, RESOURCE_PATH, CHUNK_SIZE

import warnings

warnings.filterwarnings(
	"ignore",
	category=DeprecationWarning,
	module=r"langchain_core\.tools",
)

load_dotenv()
CHATGPT_API_KEY = os.getenv("CHATGPT_API_KEY")
SERPER_API_KEY = os.getenv("SERPER_API_KEY")


# PDF ë¡œë“œ
def load_pdfs(pdf_dir):
    docs = []
    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]

    print(f"[INFO] ì´ PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}")

    for filename in tqdm(pdf_files, desc="ğŸ“„ PDF ë¡œë”© ì¤‘"):
        path = os.path.join(pdf_dir, filename)
        try:
            loader = PyPDFLoader(path)
            loaded = loader.load()
            docs.extend(loaded)
            print(f"[DEBUG] {filename} â†’ ë¬¸ì„œ {len(loaded)}ê°œ ë¡œë“œ")
        except Exception as e:
            print(f"[ERROR] {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")

    return docs

def load_txts(txt_dir):
    docs = []
    for filename in os.listdir(txt_dir):
        if filename.endswith(".txt"):
            path = os.path.join(txt_dir, filename)
            print(f"[DEBUG] ì‹œë„ ì¤‘: {path}")
            try:
                loader = TextLoader(path, encoding="utf-8")
                doc = loader.load()
                print(f"[DEBUG] ë¡œë“œ ì„±ê³µ: {filename}, ë¬¸ì„œ ìˆ˜: {len(doc)}")
                docs.extend(doc)
            except Exception as e:
                print(f"[ERROR] {filename} ë¡œë”© ì‹¤íŒ¨: {e}")
    return docs

# í…ìŠ¤íŠ¸ ë¶„í•  (RecursiveCharacterTextSplitter ì‚¬ìš©)
def split_documents(documents):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", " "]  # ë¬¸ë‹¨, ë¬¸ì¥ ë‹¨ìœ„ ìš°ì„  ë¶„í• 
    )

    all_chunks = []
    for doc in tqdm(documents, desc="ğŸ“š ë¬¸ì„œ ë¶„í•  ì¤‘"):
        chunks = splitter.split_documents([doc])
        all_chunks.extend(chunks)

    return all_chunks

def brndStatsCsvToDocs(datBrnd, datFchhq, brandStats):
    docs = []
    brandStats = brandStats.merge(datBrnd[["brnd_nm", 'brnd_no', 'fchhq_no', 'uj3_nm']], on=['brnd_no', 'fchhq_no'], how='left')
    brandStats = brandStats.merge(datFchhq[["fchhq_nm", 'fchhq_no']], on='fchhq_no', how='left')

    ## float ì»¬ëŸ¼ objectë¡œ ìºìŠ¤íŒ…
    for col in brandStats.columns:
        if brandStats[col].dtype == "float64":
            brandStats[col] = brandStats[col].astype("object")

    brandStats = brandStats.where(pd.notnull(brandStats), None)
    brandStatsVoList: list[brandStatsVo] = [
        brandStatsVo(**row._asdict()) for row in brandStats.itertuples(index=False)
    ]

    for vo in brandStatsVoList:
        text = f"""
                {vo.ym_date} ê¸°ì¤€, í”„ëœì°¨ì´ì¦ˆ {vo.fchhq_nm} ë¸Œëœë“œì˜ {vo.brnd_nm} ì •ë³´ëŠ” ì—…ì¢… ì†Œë¶„ë¥˜ëŠ” {vo.uj3_nm} ì—…ì¢…ì½”ë“œ {vo.uj3_cd}, í”„ëœì°¨ì´ì¦ˆ ì½”ë“œ {vo.fchhq_no},
                ë¸Œëœë“œ ì½”ë“œ {vo.brnd_no}ì— í•´ë‹¹í•˜ëŠ” ë¸Œëœë“œì˜ ê°œì  ìˆ˜ëŠ” {vo.store_open_cnt or 0}ê°œ,
                íì  ìˆ˜ëŠ” {vo.store_close_cnt or 0}ê°œì´ë©° ì „ì²´ ì í¬ ìˆ˜ëŠ” {vo.store_cnt or 0}ê°œì…ë‹ˆë‹¤.
                NICE ê¸°ì¤€ ê°€ë§¹ì  ìˆ˜ëŠ” {vo.store_cnt_nice or 0}ê°œì´ê³ ,
                í‰ê·  ë§¤ì¶œì€ {vo.amt_avg or 0}ì²œì›, ì´ ë§¤ì¶œì€ {vo.amt_total or 0}ì²œì›ì…ë‹ˆë‹¤.
                """
        doc = Document(
            page_content = re.sub(r"\s+", " ", text).strip(),
            metadata = {k: (v if v is not None else 0) for k, v in asdict(vo).items()}
        )
        docs.append(doc)
    return docs

def datStoreVoToDocs(datStore, cdDong, datBrnd, datFchhq, cdGps):
    docs = []

    ## ì „ì²˜ë¦¬
    datBrnd = datBrnd.rename(columns={"rprsv_nm": "brnd_rprsv_nm"}, inplace=False)
    datFchhq = datFchhq.rename(columns={"rprsv_nm": "fchhq_rprsv_nm"}, inplace=False)
    datBrnd = datBrnd.rename(columns={"y_ftc": "brnd_y_ftc"}, inplace=False)
    datFchhq = datFchhq.rename(columns={"y_ftc": "fchhq_y_ftc"}, inplace=False)
    cdDong = cdDong.drop(columns=['mega_nm', 'cty_nm']) # ì¹¼ëŸ¼ ì‚­ì œ
    datStore = datStore.dropna(subset=['fchhq_no', 'brnd_no']) # ê°’ì´ ë¹„ì—ˆë‹¤ë©´ í–‰ ì‚­ì œ
    datStore = datStore.drop(columns=['uj3_cd']) # ì¹¼ëŸ¼ ì‚­ì œ
    datBrnd['ymd_brnd'] = datBrnd['ymd_brnd'].apply(
        lambda x: str(int(x)) if pd.notna(x) and isinstance(x, float) else str(x)
    )
    datStore["ym_start"] = datStore["ym_start"].apply(
        lambda x: str(int(x)) if pd.notna(x) and isinstance(x, float) else str(x)
    )

    datStore["ym_end"] = datStore["ym_end"].apply(
        lambda x: str(int(x)) if pd.notna(x) and isinstance(x, float) else str(x)
    )

    datStore = datStore.merge(cdGps, on="pnu", how="left")
    datStore = datStore.merge(cdDong, on="dong_cd", how="left")
    datStore = datStore.merge(datBrnd, on=['brnd_no', 'fchhq_no'], how="left")
    datStore = datStore.merge(datFchhq, on='fchhq_no', how="left")

    ## float ì»¬ëŸ¼ objectë¡œ ìºìŠ¤íŒ…
    for col in datStore.columns:
        if datStore[col].dtype == "float64":
            datStore[col] = datStore[col].astype("object")

    datStore = datStore.where(pd.notnull(datStore), None)
    allowed_keys = {f.name for f in fields(datStoreVo)}
    datStoreVoList: list[datStoreVo] = [
        datStoreVo(**{k: v for k, v in row._asdict().items() if k in allowed_keys}) for row in datStore.itertuples(index=False)
    ]

    for vo in datStoreVoList:
        text = f"""
            í•´ë‹¹ ê°€ë§¹ì ì€ {vo.ym_start}ë¶€í„° ìš´ì˜ì„ ì‹œì‘í–ˆìœ¼ë©°,
            ìœ„ì¹˜ëŠ” {vo.zone_nm}ì— ì†í•˜ê³ , ìë©´ë™ ì½”ë“œëŠ” {vo.dong_cd}ì…ë‹ˆë‹¤.
            
            ì§€ë¦¬ì ìœ¼ë¡œëŠ” ìœ„ë„ {vo.gps_lat}Â°, ê²½ë„ {vo.gps_lon}Â°ì— ìœ„ì¹˜í•´ ìˆìœ¼ë©°,
            í•„ì§€ê³ ìœ ë²ˆí˜¸ëŠ” {vo.pnu}ì…ë‹ˆë‹¤.
            
            ì´ ê°€ë§¹ì ì€ {vo.fchhq_nm} í”„ëœì°¨ì´ì¦ˆ ì†Œì†ìœ¼ë¡œ, í”„ëœì°¨ì´ì¦ˆ ì½”ë“œëŠ” {vo.fchhq_no}, ëŒ€í‘œìëŠ” {vo.fchhq_rprsv_nm or 'ì •ë³´ì—†ìŒ'}ì…ë‹ˆë‹¤.
            í•´ë‹¹ í”„ëœì°¨ì´ì¦ˆì˜ ë¸Œëœë“œëŠ” {vo.brnd_nm}(ë¸Œëœë“œ ì½”ë“œ: {vo.brnd_no})ì´ë©°, ë¸Œëœë“œ ëŒ€í‘œëŠ” {vo.brnd_rprsv_nm or 'ì •ë³´ì—†ìŒ'}ì…ë‹ˆë‹¤.
            
            ë¸Œëœë“œëŠ” {vo.ymd_brnd}ì— ê°€ë§¹ì‚¬ì—…ì„ ê°œì‹œí–ˆìœ¼ë©°,
            ì—…ì¢… ì†Œë¶„ë¥˜ëŠ” {vo.uj3_nm}(ì—…ì¢… ì½”ë“œ: {vo.uj3_cd})ì— í•´ë‹¹í•©ë‹ˆë‹¤.
            
            {'í•´ë‹¹ ì§€ì ì€ í˜„ì¬ê¹Œì§€ ìš´ì˜ ì¤‘ì¸ ê°€ë§¹ì ì…ë‹ˆë‹¤.' if vo.ym_end == '202502' else f'í•´ë‹¹ ì§€ì  íì—…ì¼ì€ {vo.ym_end}ì…ë‹ˆë‹¤.'}
            """
        doc = Document(
            page_content = re.sub(r"\s+", " ", text).strip(),
            metadata = {k: (v if v is not None else 0) for k, v in asdict(vo).items()}
        )
        docs.append(doc)

    return docs

def datSalesVoToDocs(datSales, datBrnd, datFchhq, cdDong):
    docs = []
    datSales = datSales.merge(datBrnd[['brnd_nm', 'brnd_no', 'fchhq_no']], on=['brnd_no', 'fchhq_no'], how="left")
    datSales = datSales.merge(datFchhq[['fchhq_nm', 'fchhq_no']], on='fchhq_no', how="left")
    datSales = datSales.merge(cdDong, on='dong_cd', how="left")

    ## float ì»¬ëŸ¼ objectë¡œ ìºìŠ¤íŒ…
    for col in datSales.columns:
        if datSales[col].dtype == "float64":
            datSales[col] = datSales[col].astype("object")

    datSales = datSales.where(pd.notnull(datSales), None)
    allowed_keys = {f.name for f in fields(datSalesVo)}
    datSalesVoList: list[datSalesVo] = [
        datSalesVo(**{k: v for k, v in row._asdict().items() if k in allowed_keys}) for row in datSales.itertuples(index=False)
    ]

    for vo in datSalesVoList:
        zone_text = format_region_stats(
            prefix="ìë©´ë™",
            name=vo.zone_nm,
            cnt=vo.zone_cnt,
            avg=vo.zone_amt_avg,
            p25=vo.zone_amt_25pct,
            p50=vo.zone_amt_50pct,
            p75=vo.zone_amt_75pct,
            percnt=vo.zone_amt_percnt
        )

        cty_text = format_region_stats(
            prefix="ì‹œêµ°êµ¬",
            name=vo.cty_nm,
            cnt=vo.cty_cnt,
            avg=vo.cty_amt_avg,
            p25=vo.cty_amt_25pct,
            p50=vo.cty_amt_50pct,
            p75=vo.cty_amt_75pct,
            percnt=vo.cty_amt_percnt
        )

        mega_text = format_region_stats(
            prefix="ê´‘ì—­ì‹œë„",
            name=vo.mega_nm,
            cnt=vo.mega_cnt,
            avg=vo.mega_amt_avg,
            p25=vo.mega_amt_25pct,
            p50=vo.mega_amt_50pct,
            p75=vo.mega_amt_75pct,
            percnt=vo.mega_amt_percnt
        )

        all_text = format_region_stats(
            prefix="ì „êµ­",
            name="ì „ì²´",
            cnt=vo.all_cnt,
            avg=vo.all_amt_avg,
            p25=vo.all_amt_25pct,
            p50=vo.all_amt_50pct,
            p75=vo.all_amt_75pct,
            percnt=vo.all_amt_percnt
        )

        # ë³¸ì‚¬/ë¸Œëœë“œ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
        has_brand = vo.fchhq_no is not None and vo.brnd_no is not None

        # ë¶„ê¸° ì²˜ë¦¬
        if has_brand:
            header_text = f"""
        {vo.ym_sales} ê¸°ì¤€, ë³¸ì‚¬ {vo.fchhq_nm} (ID: {vo.fchhq_no})ì™€ ë¸Œëœë“œ {vo.brnd_nm} (ID: {vo.brnd_no})ëŠ”
        ì—…ì¢… ì†Œë¶„ë¥˜ '{vo.uj3_nm}' (ì½”ë“œ: {vo.uj3_cd})ì— ì†í•©ë‹ˆë‹¤.
        """
        else:
            header_text = f"""
        {vo.ym_sales} ê¸°ì¤€, ì—…ì¢… ì†Œë¶„ë¥˜ '{vo.uj3_nm}' (ì½”ë“œ: {vo.uj3_cd})ì— ì†í•˜ëŠ” ì „ì²´ ê°€ë§¹ì  ë° ë¸Œëœë“œë¥¼ í¬í•¨í•œ ë§¤ì¶œ í†µê³„ì…ë‹ˆë‹¤.
        íŠ¹ì • ë³¸ì‚¬ ë˜ëŠ” ë¸Œëœë“œì™€ëŠ” ì—°ê²°ë˜ì§€ ì•Šì€ ì¼ë°˜ ê°€ë§¹ì  ë°ì´í„°ë„ í¬í•¨ë©ë‹ˆë‹¤.
        """

        # ì „ì²´ ë¬¸ì¥ ì¡°ë¦½
        full_text = f"""
        {header_text}
        {zone_text}{cty_text}{mega_text}{all_text}
        """

        # Document ìƒì„±
        doc = Document(
            page_content=re.sub(r"\s+", " ", full_text).strip(),
            metadata={k: (v if v is not None else 0) for k, v in asdict(vo).items()}
        )
        docs.append(doc)

        # text = f"""
        # {vo.ym_sales} ê¸°ì¤€, ë³¸ì‚¬ {vo.fchhq_nm} (ID: {vo.fchhq_no})ì™€ ë¸Œëœë“œ {vo.brnd_nm} (ID: {vo.brnd_no})ëŠ”
        # ì—…ì¢… ì†Œë¶„ë¥˜ '{vo.uj3_nm}' (ì½”ë“œ: {vo.uj3_cd})ì— ì†í•©ë‹ˆë‹¤.
        #
        # {zone_text}{cty_text}{mega_text}{all_text}
        # """
        # doc = Document(
        #     page_content=re.sub(r"\s+", " ", text).strip(),
        #     metadata={k: (v if v is not None else 0) for k, v in asdict(vo).items()}
        # )
        # docs.append(doc)

    return docs

def format_region_stats(prefix: str, name: str, cnt, avg, p25, p50, p75, percnt) -> str:
    if cnt is None or cnt < 5:
        return ""

    lines = [f"- {prefix}({name}) ë‹¨ìœ„ì—ì„œëŠ” ì´ {int(cnt)}ê°œ ê°€ë§¹ì ì´ ìš´ì˜ ì¤‘ì´ë©°,"]

    if cnt >= 5:
        lines.append(f"  í‰ê·  ë§¤ì¶œì€ {int(avg) if avg is not None else 0}ì²œì›,")
        lines.append(f"  ê±´ë‹¹ í‰ê·  ë§¤ì¶œì€ {int(percnt) if percnt is not None else 0}ì²œì›ì…ë‹ˆë‹¤.")

    if cnt >= 10:
        lines.insert(-1, f"  ìƒìœ„ 25%ëŠ” {int(p25) if p25 is not None else 0}ì²œì›,")
        lines.insert(-1, f"  ìƒìœ„ 50%ëŠ” {int(p50) if p50 is not None else 0}ì²œì›,")
        lines.insert(-1, f"  ìƒìœ„ 75%ëŠ” {int(p75) if p75 is not None else 0}ì²œì›,")

    return "\n".join(lines) + "\n"

# CSV ë°ì´í„° ê°€ê³µ ë° ë¡œë“œ
def load_csvs():
    docs = []

    logging.info("ì—‘ì…€ ë°ì´í„° ë¡œë“œ..")
    brandStats = pd.read_csv(DATA_PATH / "brand_stats_v2.csv", encoding='utf-8')
    datStore = pd.read_csv(DATA_PATH / "dat_store_v2.csv", encoding='utf-8')
    datSales = pd.read_csv(DATA_PATH / "dat_sales_v3.csv", encoding='utf-8')

    cdDong = pd.read_csv(DATA_PATH / "cd_dong_v2.csv", encoding='utf-8')
    datBrnd = pd.read_csv(DATA_PATH / "dat_brnd_v2.csv", encoding='utf-8')
    datFchhq = pd.read_csv(DATA_PATH / "dat_fchhq_v2.csv", encoding='utf-8')
    cdGps = pd.read_csv(DATA_PATH / "cd_gps_v2.csv", encoding='utf-8')

    # DAT_STORE ê°€ë§¹ì  ì •ë³´ í…Œì´ë¸” DOCS
    logging.info("DAT_STORE ê°€ë§¹ì  ì •ë³´ í…Œì´ë¸” DOCS ì²˜ë¦¬ì¤‘..")
    docs.extend(datStoreVoToDocs(datStore, cdDong, datBrnd, datFchhq, cdGps))

    # BRAND_STATS ë¸Œëœë“œ í†µê³„ í…Œì´ë¸” DOCS
    logging.info("BRAND_STATS ë¸Œëœë“œ í†µê³„ í…Œì´ë¸” DOCS ì²˜ë¦¬ì¤‘..")
    docs.extend(brndStatsCsvToDocs(datBrnd, datFchhq, brandStats))

    # DAT_SALES ë§¤ì¶œ í†µê³„ í…Œì´ë¸” DOCS
    logging.info("DAT_SALES ë§¤ì¶œ í†µê³„ í…Œì´ë¸” DOCS ì²˜ë¦¬ì¤‘..")
    docs.extend(datSalesVoToDocs(datSales, datBrnd, datFchhq, cdDong))

    del brandStats
    del datStore
    del datSales

    del cdDong
    del datBrnd
    del datFchhq
    del cdGps

    gc.collect()

    return docs

# 4. ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
def build_chroma_vectorstore(pdf_dir, txt_dir, persist_dir):
    # (1) ë¡œë”© ë°ì´í„° ì…‹íŒ…
    logging.info("MAIN ë°ì´í„° ë¡œë“œ ì‹œì‘..")
    documents = load_pdfs(pdf_dir)
    documents.extend(load_txts(txt_dir))
    # documents = load_csvs()

    print(f"[DEBUG] ë¬¸ì„œ ìˆ˜: {len(documents)}")

    # (2) í…ìŠ¤íŠ¸ ë¶„í• 
    logging.info("MAIN í…ìŠ¤íŠ¸ ë¶„í•  SPLIT")
    split_docs = split_documents(documents)

    # (3) OpenAI ì„ë² ë”©
    logging.info("OpenAI ì„ë² ë”© ì‹œì‘")
    embedding = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=CHATGPT_API_KEY,
        chunk_size=100  # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì´ìŠˆë¡œ ì˜µì…˜ ì¶”ê°€
    )

    # (4) Chroma DB ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    total = len(split_docs)

    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        logging.info("ê¸°ì¡´ Chroma DB ê°ì§€, ë²¡í„° ì¶”ê°€ ì¤‘...")
        vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding)

        for i in tqdm(range(0, total, CHUNK_SIZE)):
            chunk = split_docs[i:i + CHUNK_SIZE]
            if chunk:  # ì•ˆì „ í™•ì¸
                vectordb.add_documents(chunk)

    else:
        logging.info("ì‹ ê·œ Chroma DB ìƒì„± ì¤‘...")

        if total == 0:
            raise ValueError("âŒ split_docsê°€ ë¹„ì–´ ìˆì–´ Chromaë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì²« ë°°ì¹˜ë¡œ ì´ˆê¸° ìƒì„±
        first_batch = split_docs[:CHUNK_SIZE]
        vectordb = Chroma.from_documents(
            documents=first_batch,
            embedding=embedding,
            persist_directory=persist_dir
        )

        # ë‚¨ì€ ë°°ì¹˜ ì¶”ê°€
        for i in tqdm(range(CHUNK_SIZE, total, CHUNK_SIZE)):
            chunk = split_docs[i:i + CHUNK_SIZE]
            if chunk:  # ë°©ì–´ì  ì²˜ë¦¬
                vectordb.add_documents(chunk)

    logging.info(f"âœ… ì´ {len(split_docs)} chunks ì €ì¥ ì™„ë£Œ!")


# ì‹¤í–‰
if __name__ == "__main__":
    build_chroma_vectorstore(pdf_dir="./pdf", txt_dir="./txt",persist_dir="./chroma_db")